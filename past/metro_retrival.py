# -*- coding: utf-8 -*-
"""Metro_retrival.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/119aazeXGJEDzu-mkPIcElT91GKOpNXYm

# Setting up
"""
# Setting up 
## Installing libraries 
!pip install tensorflow-recommenders --no-deps

!pip install tensorflow==2.11.0

## Loading libraries 
import pandas as pd
import tensorflow as tf
import os
import glob
import numpy as np
import tensorflow_recommenders as tfrs
import matplotlib.pyplot as plt
from typing import Dict, Text
import datetime as dt
import pickle
import random

print(tfrs.__version__)
print(tf.__version__)

"""# Data"""

result = pd.read_csv('/content/drive/MyDrive/Work/C4R/Metro/Data/ml_300.csv')

result.shape

print(result['city'].unique())

# result = result.loc[(result['city'] == 'KYIV')]

result[['cust_person_id', 'product_id']] = result[['cust_person_id', 'product_id']].astype(str)
result[['visits', 'sell_total_val_nsp', 'sell_val_nsp']] = result[['visits', 'sell_total_val_nsp', 'sell_val_nsp']].fillna(0)
result[['visits', 'sell_total_val_nsp', 'sell_val_nsp']] = result[['visits', 'sell_total_val_nsp', 'sell_val_nsp']].astype(int)
result[['SEGMENT_GROUP']] = result[['SEGMENT_GROUP']].fillna('unknown')

print(result.shape)
result.tail()

result['pcg_cat_desc_tl'].nunique()

result.dtypes

product_df = result.groupby('product_id')['sell_val_nsp'].sum().reset_index()

product_df = product_df.sort_values(by='sell_val_nsp', ascending=False)

# Calculate the cumulative percentage of sales
product_df['cumulative_percentage'] = (product_df['sell_val_nsp'].cumsum() / product_df['sell_val_nsp'].sum()) * 100

# Create the Pareto chart
plt.figure(figsize=(20, 6))
plt.bar(product_df['product_id'], product_df['sell_val_nsp'], color='blue')
plt.plot(product_df['product_id'], product_df['cumulative_percentage'], color='red', marker='o')

# Add labels and title
plt.xlabel('Product ID')
plt.ylabel('Sales')
plt.title('Pareto Chart: Product Contribution to Total Sales')

# Grid and annotations
plt.grid(True, linestyle='--')
plt.annotate('80/20 Rule', xy=(product_df['product_id'][1], product_df['cumulative_percentage'][1]), xytext=(10, 10),
             arrowprops=dict(facecolor='black', shrink=0.05), fontsize=12)

plt.show()

temp_df = product_df[(product_df['cumulative_percentage'] <= 100)]

print(temp_df.shape, product_df.shape)
print(temp_df['product_id'].nunique(), product_df['product_id'].nunique())

temp_df.tail()

product_list = list(set(temp_df['product_id'].to_list()))

print(len(product_list))

filtered_df = result[result['product_id'].isin(product_list)]

print(filtered_df.shape, result.shape)

filtered_df['product_id'].nunique()

filtered_df.tail()

filtered_df['pcg_cat_desc_tl'].nunique()

"""# TF records"""

def save_to_tf_records(df, output_dir, rows_per_file=1000):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    for i in range(0, len(df), rows_per_file):
        chunk = df[i:i+rows_per_file]
        filename = f"data_{i:05d}.tfrecord"
        filepath = os.path.join(output_dir, filename)
        with tf.io.TFRecordWriter(filepath) as writer:
            for _, row in chunk.iterrows():
                example = tf.train.Example(features=tf.train.Features(feature={
                    'customer_id': tf.train.Feature(bytes_list=tf.train.BytesList(value=[row["cust_person_id"].encode()])),
                    'product_id': tf.train.Feature(bytes_list=tf.train.BytesList(value=[row["product_id"].encode()])),
                    "revenue": tf.train.Feature(int64_list=tf.train.Int64List(value=[row["sell_total_val_nsp"]])),
                    'city': tf.train.Feature(bytes_list=tf.train.BytesList(value=[row["city"].encode()])),
                    }))

                writer.write(example.SerializeToString())

save_to_tf_records(filtered_df, "tf_recs", rows_per_file=200_000)

"""# TF Dataset"""

feature_description = {
    'product_id': tf.io.FixedLenFeature([], tf.string, default_value=''),
    'customer_id': tf.io.FixedLenFeature([], tf.string, default_value=''),
    # 'segment': tf.io.FixedLenFeature([], tf.string, default_value=''),
    'city': tf.io.FixedLenFeature([], tf.string, default_value=''),
    # 'articul': tf.io.FixedLenFeature([], tf.string, default_value=''),
    # 'category': tf.io.FixedLenFeature([], tf.string, default_value=''),
    # 'sub_category': tf.io.FixedLenFeature([], tf.string, default_value=''),
    # 'visits': tf.io.FixedLenFeature([], tf.int64, default_value=0),
    'revenue': tf.io.FixedLenFeature([], tf.int64, default_value=0),
    # 'ts': tf.io.FixedLenFeature([], tf.int64, default_value=0),
    # 'rating': tf.io.FixedLenFeature([], tf.int64, default_value=0),
 }

def _parse_function(example_proto):
  return tf.io.parse_single_example(example_proto, feature_description)

tfrecord_files = glob.glob('tf_recs/*')

full_ds = tf.data.TFRecordDataset(tfrecord_files).map(_parse_function)

for elem in full_ds.take(1):
    print(elem)

client_ds = full_ds.map(lambda x: {
    "product_id": x["product_id"],
    'customer_id': x['customer_id'],
    # "segment": x['segment'],
    'city': x['city'],
    # 'visits': x['visits'],
    'revenue': x['revenue'],
    # 'category': x['category'],
    # 'sub_category': x['sub_category'],
    # 'articul': x['articul'],
    # 'ts': x['ts'],
    # 'rating': x['rating'],
})

for elem in client_ds.take(2):
    print(elem)

unique_product_ids_df = pd.DataFrame(filtered_df['product_id'].unique(), columns=['product_id'])

product_ids = {key: col.values for key, col in dict(unique_product_ids_df[['product_id']]).items()}

product_ds = tf.data.Dataset.from_tensor_slices(product_ids)

for elem in product_ds.take(2):
    print(elem)

"""# Models Retrival

## Vocabs
"""

revenues = np.concatenate(list(client_ds.map(lambda x: x["revenue"]).batch(1_000)))
max_revenues = revenues.max()
min_revenues = revenues.min()

revenues_buckets = np.linspace(
    min_revenues, max_revenues, num=100,
)
unique_revenues = np.unique(np.concatenate(list(client_ds.batch(1_000).map(
                                                  lambda x: x['revenue']))))

print(len(unique_revenues))
print(unique_revenues[:10])

unique_product_ids = np.unique(np.concatenate(list(product_ds.batch(1_000).map(
    lambda x: x["product_id"]))))

print(len(unique_product_ids))
print(unique_product_ids[:10])

unique_customer_id = np.unique(np.concatenate(list(client_ds.batch(1_000).map(
    lambda x: x["customer_id"]))))

print(len(unique_customer_id))
print(unique_customer_id[:10])

unique_city = np.unique(np.concatenate(list(client_ds.batch(1_000).map(
    lambda x: x["city"]))))

print(len(unique_city))
print(unique_city)

# unique_category = np.unique(np.concatenate(list(client_ds.batch(1_000).map(
#      lambda x: x["category"]))))

# print(len(unique_category))
# print(unique_category[:10])

"""## Buyer Model"""

class BuyerModel(tf.keras.Model):

  def __init__(self):
    super().__init__()

    self.customers_embedding = tf.keras.Sequential([
       tf.keras.layers.StringLookup(
         vocabulary = unique_customer_id, mask_token = None),
         tf.keras.layers.Embedding(len(unique_customer_id) + 1, 32),
     ])

    self.city_embedding = tf.keras.Sequential([
       tf.keras.layers.StringLookup(
         vocabulary = unique_city, mask_token = None),
         tf.keras.layers.Embedding(len(unique_city) + 1, 32),
     ])

    # self.category_embedding = tf.keras.Sequential([
    #    tf.keras.layers.StringLookup(
    #      vocabulary = unique_category, mask_token = None),
    #      tf.keras.layers.Embedding(len(unique_category) + 1, 32),
    #  ])

    self.revenues_embedding = tf.keras.Sequential([
          tf.keras.layers.Discretization(revenues_buckets.tolist()),
          tf.keras.layers.Embedding(len(revenues_buckets) + 1, 32),])
    self.normalized_revenues = tf.keras.layers.Normalization(axis=None)
    self.normalized_revenues.adapt(unique_revenues)

    # self.visits_embedding = tf.keras.Sequential([
    #       tf.keras.layers.Discretization(visits_buckets.tolist()),
    #       tf.keras.layers.Embedding(len(visits_buckets) + 1, 32),])
    # self.normalized_visits = tf.keras.layers.Normalization(axis=None)
    # self.normalized_visits.adapt(unique_visits)

  def call(self, inputs):
    return tf.concat([
      self.customers_embedding(inputs['customer_id']),
      self.city_embedding(inputs['city']),
      # self.category_embedding(inputs['category']),
      self.revenues_embedding(inputs["revenue"]),
      tf.reshape(self.normalized_revenues(inputs["revenue"]), (-1, 1)),
      # self.visits_embedding(inputs["visits"]),
      # tf.reshape(self.normalized_visits(inputs["visits"]), (-1, 1)),
      ], axis=1)

buyer_model = BuyerModel()

for row in client_ds.batch(1).take(1):
  print(f"Computed representations: {buyer_model(row)[0, :]}")

"""## Product model"""

class ProductModel(tf.keras.Model):

  def __init__(self):
    super().__init__()

    self.product_embedding = tf.keras.Sequential([
        tf.keras.layers.StringLookup(
            vocabulary=unique_product_ids, mask_token=None),
        tf.keras.layers.Embedding(len(unique_product_ids) + 1, 32),
    ])

  def call(self, inputs):
    return tf.concat([
      self.product_embedding(inputs['product_id'])
      ], axis=1)

product_model = ProductModel()

for row in product_ds.batch(10).take(1):
    # print(row)
  print(f"Computed representations: {product_model(row)[0, :]}")

"""## Joint Model"""

class Retrival(tfrs.models.Model):

  def __init__(self):
    super().__init__()

    self.query_model = tf.keras.Sequential([
      BuyerModel(),
      tf.keras.layers.Dense(64, kernel_regularizer= tf.keras.regularizers.l1(0.01)),
      tf.keras.layers.Dense(32),
      tf.keras.layers.Dense(16),
      # tf.keras.layers.Dense(8)
       ])

    self.candidate_model = tf.keras.Sequential([
      ProductModel(),
      tf.keras.layers.Dense(64, kernel_regularizer= tf.keras.regularizers.l1(0.01)),
      tf.keras.layers.Dense(32),
      tf.keras.layers.Dense(16),
      # tf.keras.layers.Dense(8)
      ])

    self.retrieval_task: tf.keras.layers.Layer = tfrs.tasks.Retrieval(
        metrics=tfrs.metrics.FactorizedTopK(
            candidates=product_ds.batch(128).map(self.candidate_model)
        )
    )

  def call(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:
    buyer_embeddings = self.query_model({
      'customer_id': features['customer_id'],
      # 'category': features['category'],
      'city': features['city'],
      'revenue': features['revenue'],
      # 'visits': features['visits'],
      })

    product_embeddings = self.candidate_model({'product_id': features["product_id"]})

    return buyer_embeddings, product_embeddings


  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:
    buyer_embeddings, product_embeddings = self(features)

    retrieval_loss = self.retrieval_task(buyer_embeddings, product_embeddings)

    return retrieval_loss

model_retrival = Retrival()

iterator = client_ds.shuffle(buffer_size=1_000).take(1).as_numpy_iterator()

# Iterate over the elements and extract values
for elem in iterator:
    buyer_info = {
        'customer_id': np.array([elem['customer_id']]),
        'city': np.array([elem['city']]),
        # 'category': np.array([elem['category']]),
        'revenue': np.array([elem['revenue']]),
    }

print(model_retrival.query_model(buyer_info))

iterator = product_ds.shuffle(buffer_size=1_000).take(1).as_numpy_iterator()
for elem in iterator:
    product_detail = {
        'product_id': np.array([elem['product_id']])
    }

print(model_retrival.candidate_model(product_detail))

"""## Training"""

tf.random.set_seed(158)
shuffled = client_ds.shuffle(100_000, seed=42, reshuffle_each_iteration=False)

train = shuffled.take(7_500_000)
test = shuffled.skip(7_500_000)

cached_train = train.batch(10_000).cache()
cached_test = test.batch(5_000).cache()

MAX_EPOCHS = 50

def compile_and_fit(model, train_ds, test_ds, patience=3):
   #early_stopping = tf.keras.callbacks.EarlyStopping(monitor='factorized_top_k/top_5_categorical_accuracy',
  #                                                   patience=patience,
  #                                                   mode='min')

  log_dir = "/content/drive/MyDrive/Work/C4R/Metro/logs/fit/" + dt.datetime.now().strftime("%Y%m%d-%H%M%S")
  # log_dir = "" + dt.datetime.now().strftime("%Y%m%d-%H%M%S")
  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

  model.compile(optimizer= tf.keras.optimizers.Adagrad(0.1))

  history = model.fit(train_ds, epochs=MAX_EPOCHS,
                      callbacks = [tensorboard_callback])
  metrics = model.evaluate(test_ds, return_dict=True)

  return model, history, metrics

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# model, history, metrics = compile_and_fit(model_retrival, cached_train, cached_test)
# 
# index = tfrs.layers.factorized_top_k.BruteForce(model.query_model, k = 100)
# 
# index.index_from_dataset(
#     tf.data.Dataset.zip((product_ds.map(lambda x: x['product_id']).batch(128), product_ds.batch(128).map(model.candidate_model)))
# )
# 
# score, title = index(buyer_info)
# 
# a = np.dstack((title, score))
# 
# print(a)
# 
# path = os.path.join('/content/drive/MyDrive/Work/C4R/Metro/Models', "UA_P2C_all_sku")
# 
# tf.saved_model.save(index, path)
# 
# loaded = tf.saved_model.load(path)
# loaded(buyer_info)
# 
# plt.plot(history.history['factorized_top_k/top_5_categorical_accuracy'])
# plt.plot(history.history['factorized_top_k/top_10_categorical_accuracy'])
# plt.plot(history.history['factorized_top_k/top_50_categorical_accuracy'])
# plt.plot(history.history['factorized_top_k/top_100_categorical_accuracy'])

plt.plot(history.history['factorized_top_k/top_5_categorical_accuracy'])
plt.plot(history.history['factorized_top_k/top_10_categorical_accuracy'])
plt.plot(history.history['factorized_top_k/top_50_categorical_accuracy'])
plt.plot(history.history['factorized_top_k/top_100_categorical_accuracy'])

"""## Testing model"""

path = '/content/drive/MyDrive/Work/C4R/Metro/Models/complex_input'

loaded = tf.saved_model.load(path)

loaded

buyer_info = {'customer_id': np.array([b'330067026301'], dtype='|S12'),
 'city': np.array([b'KYIV'], dtype='|S4'),
 'revenue': np.array([91389])}

buyer_info

loaded(buyer_info)

# Create an iterator over the dataset
iterator = client_ds.shuffle(buffer_size=1000).take(1).as_numpy_iterator()

# Iterate over the elements and extract values
for elem in iterator:
    buyer_info = {
        'customer_id': np.array([elem['customer_id']]),
        # 'segment': np.array([elem['segment']]),
        'city': np.array([elem['city']]),
        # 'ts': np.array([elem['ts']]),
        # 'visits': np.array([elem['visits']]),
        'revenue': np.array([elem['revenue']]),
        # 'category': np.array([elem['category']]),
        # 'articul': np.array([elem['articul']]),
        # 'sub_category': np.array([elem['sub_category']])
    }
    extracted_id = np.array(elem['product_id']).astype(int)

# product_row = articles_df[articles_df['product_id'] == extracted_id]

# print(product_row[['art_name', 'pcg_cat_desc_tl', 'pcg_sub_cat_desc']])

print(buyer_info)



"""# Model Ranking

## Vocabs
"""

revenues = np.concatenate(list(client_ds.map(lambda x: x["revenue"]).batch(1_000)))
max_revenues = revenues.max()
min_revenues = revenues.min()

revenues_buckets = np.linspace(
    min_revenues, max_revenues, num=100,
)
unique_revenues = np.unique(np.concatenate(list(client_ds.batch(1_000).map(
                                                  lambda x: x['revenue']))))

print(len(unique_revenues))
print(unique_revenues[:10])

unique_product_ids = np.unique(np.concatenate(list(product_ds.batch(1_000).map(
    lambda x: x["product_id"]))))

print(len(unique_product_ids))
print(unique_product_ids[:10])

unique_customer_id = np.unique(np.concatenate(list(client_ds.batch(1_000).map(
    lambda x: x["customer_id"]))))

print(len(unique_customer_id))
print(unique_customer_id[:10])

unique_city = np.unique(np.concatenate(list(client_ds.batch(1_000).map(
    lambda x: x["city"]))))

print(len(unique_city))
print(unique_city)

"""## Buyer Model"""

class BuyerModel(tf.keras.Model):

  def __init__(self):
    super().__init__()

    self.customers_embedding = tf.keras.Sequential([
       tf.keras.layers.StringLookup(
         vocabulary = unique_customer_id, mask_token = None),
         tf.keras.layers.Embedding(len(unique_customer_id) + 1, 32),
     ])

    self.city_embedding = tf.keras.Sequential([
       tf.keras.layers.StringLookup(
         vocabulary = unique_city, mask_token = None),
         tf.keras.layers.Embedding(len(unique_city) + 1, 32),
     ])

    self.revenues_embedding = tf.keras.Sequential([
          tf.keras.layers.Discretization(revenues_buckets.tolist()),
          tf.keras.layers.Embedding(len(revenues_buckets) + 1, 32),])
    self.normalized_revenues = tf.keras.layers.Normalization(axis=None)
    self.normalized_revenues.adapt(unique_revenues)

    # self.visits_embedding = tf.keras.Sequential([
    #       tf.keras.layers.Discretization(visits_buckets.tolist()),
    #       tf.keras.layers.Embedding(len(visits_buckets) + 1, 32),])
    # self.normalized_visits = tf.keras.layers.Normalization(axis=None)
    # self.normalized_visits.adapt(unique_visits)

  def call(self, inputs):
    return tf.concat([
      self.customers_embedding(inputs['customer_id']),
      self.city_embedding(inputs['city']),
      self.revenues_embedding(inputs["revenue"]),
      tf.reshape(self.normalized_revenues(inputs["revenue"]), (-1, 1)),
      # self.visits_embedding(inputs["visits"]),
      # tf.reshape(self.normalized_visits(inputs["visits"]), (-1, 1)),
      ], axis=1)

buyer_model = BuyerModel()

for row in client_ds.batch(1).take(1):
  print(f"Computed representations: {buyer_model(row)[0, :]}")

"""## Product model"""

class ProductModel(tf.keras.Model):

  def __init__(self):
    super().__init__()

    self.product_embedding = tf.keras.Sequential([
        tf.keras.layers.StringLookup(
            vocabulary=unique_product_ids, mask_token=None),
        tf.keras.layers.Embedding(len(unique_product_ids) + 1, 32),
    ])

  def call(self, inputs):
    return tf.concat([
      self.product_embedding(inputs['product_id'])
      ], axis=1)

product_model = ProductModel()

for row in product_ds.batch(10).take(1):
    # print(row)
  print(f"Computed representations: {product_model(row)[0, :]}")

"""## Joint Model"""

class Retrival_Ranking(tfrs.models.Model):

  def __init__(self):
    super().__init__()

    self.query_model = tf.keras.Sequential([
      BuyerModel(),
      tf.keras.layers.Dense(64, kernel_regularizer= tf.keras.regularizers.l1(0.01)),
      tf.keras.layers.Dense(32),
      tf.keras.layers.Dense(16),
             ])

    self.candidate_model = tf.keras.Sequential([
      ProductModel(),
      tf.keras.layers.Dense(64, kernel_regularizer= tf.keras.regularizers.l1(0.01)),
      tf.keras.layers.Dense(32),
      tf.keras.layers.Dense(16),
            ])

    self.retrieval_task: tf.keras.layers.Layer = tfrs.tasks.Retrieval(
        metrics=tfrs.metrics.FactorizedTopK(
            candidates=product_ds.batch(128).map(self.candidate_model)
        )
        )

    self.rating_model = tf.keras.Sequential([
        tf.keras.layers.Dense(64),
        tf.keras.layers.Dense(32),
        tf.keras.layers.Dense(1),
    ])

    self.rating_task: tf.keras.layers.Layer = tfrs.tasks.Ranking(
        loss=tf.keras.losses.MeanSquaredError(),
        metrics=[tf.keras.metrics.RootMeanSquaredError()],
    )

  def call(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:
    buyer_embeddings = self.query_model({
      'customer_id': features['customer_id'],
      'city': features['city'],
      'revenue': features['revenue'],
            })

    product_embeddings = self.candidate_model({'product_id': features["product_id"]})

    return (buyer_embeddings,
            product_embeddings,
            self.rating_model(tf.concat([buyer_embeddings, product_embeddings], axis=1)
        ),
    )

  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:
    ratings = features.pop("rating")
    buyer_embeddings, product_embeddings, rating_predictions = self(features)

    rating_loss = self.rating_task(
        labels=ratings,
        predictions=rating_predictions,
    )

    retrieval_loss = self.retrieval_task(buyer_embeddings, product_embeddings)

    return (retrieval_loss + rating_loss)

model_retrival_ranking = Retrival_Ranking()

iterator = client_ds.shuffle(buffer_size=1_000).take(1).as_numpy_iterator()

# Iterate over the elements and extract values
for elem in iterator:
    buyer_info = {
        'customer_id': np.array([elem['customer_id']]),
        'city': np.array([elem['city']]),
        # 'visits': np.array([elem['visits']]),
        'revenue': np.array([elem['revenue']]),
    }

print(model_retrival_ranking.query_model(buyer_info))

iterator = product_ds.shuffle(buffer_size=1_000).take(1).as_numpy_iterator()
for elem in iterator:
    product_detail = {
        'product_id': np.array([elem['product_id']])
    }

print(model_retrival_ranking.candidate_model(product_detail))

"""## Training"""

tf.random.set_seed(158)
shuffled = client_ds.shuffle(100_000, seed=42, reshuffle_each_iteration=False)

train = shuffled.take(3_000_000)
test = shuffled.skip(3_000_000)

cached_train = train.batch(5_000).cache()
cached_test = test.batch(2_000).cache()

MAX_EPOCHS = 200

def compile_and_fit(model, train_ds, test_ds, patience=3):
   #early_stopping = tf.keras.callbacks.EarlyStopping(monitor='factorized_top_k/top_5_categorical_accuracy',
  #                                                   patience=patience,
  #                                                   mode='min')

  log_dir = "/content/drive/MyDrive/Work/C4R/Metro/logs/fit/" + dt.datetime.now().strftime("%Y%m%d-%H%M%S")
  # log_dir = "" + dt.datetime.now().strftime("%Y%m%d-%H%M%S")
  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

  model.compile(optimizer= tf.keras.optimizers.Adagrad(0.1))

  history = model.fit(train_ds, epochs=MAX_EPOCHS,
                      callbacks = [tensorboard_callback])
  metrics = model.evaluate(test_ds, return_dict=True)

  return model, history, metrics

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# model, history, metrics = compile_and_fit(model_retrival_ranking, cached_train, cached_test)
# 
# index = tfrs.layers.factorized_top_k.BruteForce(model.query_model, k = 100)
# 
# index.index_from_dataset(
#     tf.data.Dataset.zip((product_ds.map(lambda x: x['product_id']).batch(128), product_ds.batch(128).map(model.candidate_model)))
# )
# 
# score, title = index(buyer_info)
# 
# a = np.dstack((title, score))
# 
# print(a)
# 
# path = os.path.join('/content/drive/MyDrive/Work/C4R/Metro/Models', "UA_1000_sku_retr_rank")
# 
# tf.saved_model.save(index, path)
# 
# loaded = tf.saved_model.load(path)
# loaded(buyer_info)

plt.plot(history.history['factorized_top_k/top_5_categorical_accuracy'])
plt.plot(history.history['factorized_top_k/top_10_categorical_accuracy'])
plt.plot(history.history['factorized_top_k/top_50_categorical_accuracy'])
plt.plot(history.history['factorized_top_k/top_100_categorical_accuracy'])

"""## Testing model"""

path = '/content/drive/MyDrive/Work/C4R/Metro/Models/complex_input'

loaded = tf.saved_model.load(path)

loaded

buyer_info = {'customer_id': np.array([b'330067026301'], dtype='|S12'),
 'city': np.array([b'KYIV'], dtype='|S4'),
 'revenue': np.array([91389])}

buyer_info

loaded(buyer_info)

# Create an iterator over the dataset
iterator = client_ds.shuffle(buffer_size=1000).take(1).as_numpy_iterator()

# Iterate over the elements and extract values
for elem in iterator:
    buyer_info = {
        'customer_id': np.array([elem['customer_id']]),
        # 'segment': np.array([elem['segment']]),
        'city': np.array([elem['city']]),
        # 'ts': np.array([elem['ts']]),
        # 'visits': np.array([elem['visits']]),
        'revenue': np.array([elem['revenue']]),
        # 'category': np.array([elem['category']]),
        # 'articul': np.array([elem['articul']]),
        # 'sub_category': np.array([elem['sub_category']])
    }
    extracted_id = np.array(elem['product_id']).astype(int)

# product_row = articles_df[articles_df['product_id'] == extracted_id]

# print(product_row[['art_name', 'pcg_cat_desc_tl', 'pcg_sub_cat_desc']])

print(buyer_info)

